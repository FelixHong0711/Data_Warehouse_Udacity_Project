{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Redshift Cluster using the AWS python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## STEP 1: Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imports\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create administrator IAM role on AWS (manually)\n",
    "\n",
    "\"\"\"\n",
    "- Create a new IAM user in your AWS account\n",
    "- Give it `AdministratorAccess`, from `Attach existing policies directly` Tab\n",
    "- Create access key\n",
    "- Take note of the access key and secret \n",
    "- Edit the file `dwh_ias.cfg` in the same folder as this notebook and fill\n",
    "\n",
    "KEY=YOUR_AWS_KEY\n",
    "SECRET=YOUR_AWS_SECRET\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DWH parameters from dwh_ias.cfg\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(\"dwh_ias.cfg\"))\n",
    "\n",
    "KEY = config.get(\"AWS\", \"KEY\")\n",
    "SECRET = config.get(\"AWS\", \"SECRET\")\n",
    "\n",
    "DWH_CLUSTER_TYPE = config.get(\"DWH\", \"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES = config.get(\"DWH\", \"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE = config.get(\"DWH\", \"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\", \"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB = config.get(\"DWH\", \"DWH_DB\")\n",
    "DWH_DB_USER = config.get(\"DWH\", \"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD = config.get(\"DWH\", \"DWH_DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\", \"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Param\": [\n",
    "            \"DWH_CLUSTER_TYPE\",\n",
    "            \"DWH_NUM_NODES\",\n",
    "            \"DWH_NODE_TYPE\",\n",
    "            \"DWH_CLUSTER_IDENTIFIER\",\n",
    "            \"DWH_DB\",\n",
    "            \"DWH_DB_USER\",\n",
    "            \"DWH_DB_PASSWORD\",\n",
    "            \"DWH_PORT\",\n",
    "            \"DWH_IAM_ROLE_NAME\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            DWH_CLUSTER_TYPE,\n",
    "            DWH_NUM_NODES,\n",
    "            DWH_NODE_TYPE,\n",
    "            DWH_CLUSTER_IDENTIFIER,\n",
    "            DWH_DB,\n",
    "            DWH_DB_USER,\n",
    "            DWH_DB_PASSWORD,\n",
    "            DWH_PORT,\n",
    "            DWH_IAM_ROLE_NAME,\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clients for EC2, S3, IAM, and Redshift\n",
    "region = \"us-east-1\"\n",
    "\n",
    "ec2 = boto3.resource(\n",
    "    \"ec2\", region_name=region, aws_access_key_id=KEY, aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    \"s3\", region_name=region, aws_access_key_id=KEY, aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "iam = boto3.client(\n",
    "    \"iam\", region_name=region, aws_access_key_id=KEY, aws_secret_access_key=SECRET\n",
    ")\n",
    "\n",
    "redshift = boto3.client(\n",
    "    \"redshift\", region_name=region, aws_access_key_id=KEY, aws_secret_access_key=SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## STEP 2: Create IAM role that allows Redshift to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the IAM role\n",
    "try:\n",
    "    print(\"Creating a new IAM Role\")\n",
    "    dwhRole = iam.create_role(\n",
    "        Path=\"/\",\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Action\": \"sts:AssumeRole\",\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\"Service\": \"redshift.amazonaws.com\"},\n",
    "                    }\n",
    "                ],\n",
    "                \"Version\": \"2012-10-17\",\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Successfully created a new IAM Role: {DWH_IAM_ROLE_NAME}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attaching AmazonS3ReadOnlyAccess Policy\")\n",
    "response_s3 = iam.attach_role_policy(\n",
    "    RoleName=DWH_IAM_ROLE_NAME,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "\n",
    "if response_s3 == 200:\n",
    "    print(\"Successfully attached AmazonS3ReadOnlyAccess policy to the IAM role\")\n",
    "\n",
    "print(\"Attaching AmazonRedshiftFullAccess Policy\")\n",
    "response_redshift = iam.attach_role_policy(\n",
    "    RoleName=DWH_IAM_ROLE_NAME,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonRedshiftFullAccess\"\n",
    ")[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "\n",
    "if response_redshift == 200:\n",
    "    print(\"Successfully attached AmazonRedshiftFullAccess policy to the IAM role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and print the IAM role ARN\n",
    "print(\"Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)[\"Role\"][\"Arn\"]\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  Create Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Redshift cluster subnet group\n",
    "ec2_client = boto3.client(\n",
    "    \"ec2\",\n",
    "    region_name=region,\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET,\n",
    ")\n",
    "response = ec2_client.describe_vpcs(Filters=[{\"Name\": \"isDefault\", \"Values\": [\"true\"]}])\n",
    "default_vpc_id = response[\"Vpcs\"][0][\"VpcId\"]\n",
    "\n",
    "subnet_response = ec2_client.describe_subnets(\n",
    "    Filters=[{\"Name\": \"vpc-id\", \"Values\": [default_vpc_id]}]\n",
    ")\n",
    "subnet_ids = [subnet[\"SubnetId\"] for subnet in subnet_response[\"Subnets\"]]\n",
    "subnet_group_name = \"default-vpc-redshift-subnet-group\"\n",
    "description = \"Subnet group for Redshift in default VPC\"\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster_subnet_group(\n",
    "        ClusterSubnetGroupName=subnet_group_name,\n",
    "        Description=description,\n",
    "        SubnetIds=subnet_ids,\n",
    "    )\n",
    "    print(f\"Cluster Subnet Group {subnet_group_name} created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating cluster subnet group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Redshift cluster\n",
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        # TODO: add parameters for hardware\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "        # TODO: add parameters for identifiers & credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        # TODO: add parameter for role (to allow s3 access)\n",
    "        IamRoles=[roleArn],\n",
    "        ClusterSubnetGroupName=subnet_group_name,\n",
    "    )\n",
    "    print(f\"Redshift Cluster {DWH_CLUSTER_IDENTIFIER} created successfully.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and print the Redshift cluster identifier\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    keysToShow = [\n",
    "        \"ClusterIdentifier\",\n",
    "        \"NodeType\",\n",
    "        \"ClusterStatus\",\n",
    "        \"MasterUsername\",\n",
    "        \"DBName\",\n",
    "        \"Endpoint\",\n",
    "        \"NumberOfNodes\",\n",
    "        \"VpcId\",\n",
    "    ]\n",
    "    x = [(k, v) for k, v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)[\n",
    "    \"Clusters\"\n",
    "][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DWH endpoint and IAM role ARN\n",
    "DWH_ENDPOINT = myClusterProps[\"Endpoint\"][\"Address\"]\n",
    "DWH_ROLE_ARN = myClusterProps[\"IamRoles\"][0][\"IamRoleArn\"]\n",
    "\n",
    "config[\"DWH\"][\"DWH_ENDPOINT\"] = DWH_ENDPOINT\n",
    "config[\"DWH\"][\"DWH_ROLE_ARN\"] = DWH_ROLE_ARN\n",
    "\n",
    "with open(\"dwh_ias.cfg\", \"w\") as configfile:\n",
    "    config.write(configfile)\n",
    "\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Open an incoming  TCP port to access the cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authorize default security group to allow traffic from anywhere to the Redshift port\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps[\"VpcId\"])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp=\"0.0.0.0/0\",\n",
    "        IpProtocol=\"TCP\",\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Redshift cluster connection\n",
    "conn_string = (\n",
    "    f\"postgresql://{DWH_DB_USER}:{DWH_DB_PASSWORD}@{DWH_ENDPOINT}:{DWH_PORT}/{DWH_DB}\"\n",
    ")\n",
    "print(conn_string)\n",
    "try:\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    print(\"Connection successful\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Clean up your resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>DO NOT RUN THIS UNLESS YOU ARE SURE <br/> \n",
    "    We will be using these resources in the next exercises</span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Redshift cluster\n",
    "\n",
    "redshift.delete_cluster(\n",
    "    ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Redshift cluster deletion\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)[\n",
    "    \"Clusters\"\n",
    "][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete IAM role\n",
    "iam.detach_role_policy(\n",
    "    RoleName=DWH_IAM_ROLE_NAME,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\",\n",
    ")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deakin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
